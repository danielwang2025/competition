{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f6e633d-bf7b-4b20-8fd8-5dff43fdbfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac38326-0ce6-4713-9e53-62246e98bc1b",
   "metadata": {},
   "source": [
    "### Prepare the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72146853-b1ab-472c-bb47-1c6843b77dbd",
   "metadata": {},
   "source": [
    "**Task 1.5.1:**\n",
    "\n",
    "Get the GPU device, if it is available.  Store the device name in the variable `device`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e918baa-7360-4c1d-b5cc-28ee0f5dda42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your device is: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"gpu\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"your device is: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a174dc-600f-428d-bfdf-c031f31a48ea",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1858feed-b758-4883-94f7-bd9a22bfdbd7",
   "metadata": {},
   "source": [
    "The data for this assignment is contained in the directory `sea_creatures`. In this directory, there are two folders.  We'll use `train` to train our classification model, and then submit predictions about the `test` images to the grader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26cc0819-5726-4d26-89d7-0a8c0c0fa0e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: 'sea_creatures'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msea_creatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系统找不到指定的路径。: 'sea_creatures'"
     ]
    }
   ],
   "source": [
    "os.listdir(\"sea_creatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c8719-fd9a-4872-833d-71d682f3ff47",
   "metadata": {},
   "source": [
    "**Task 1.5.2:**\n",
    "\n",
    "Find the names of the classes we will be working with.  Images for each class are inside folders within the `sea_creatures/train` folder.  Make a list of the class names (each corresponding to a directory name). Your list should be named `classes`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5185292-8d05-42b8-8de0-5d87a0464f08",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: 'sea_creatures\\\\train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m train_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msea_creatures\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(classes)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系统找不到指定的路径。: 'sea_creatures\\\\train'"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join('sea_creatures','train')\n",
    "classes = os.listdir(train_dir)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0efb39-37e0-4883-ada1-fd531afafb23",
   "metadata": {},
   "source": [
    "**Task 1.5.3:**\n",
    "\n",
    "Build a transformer pipeline.  It should ensure the images are in RGB format, scale them to 224$\\times$224 pixels, and convert them into a PyTorch tensor.  You will probably find `ConvertToRGB` useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4932c243-3478-4801-89b4-0065310dccd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    <__main__.ConvertToRGB object at 0x000001E4604FF8F0>\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "height = 224\n",
    "width = 224\n",
    "\n",
    "\n",
    "class ConvertToRGB:\n",
    "    def __call__(self, img):\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "        return img\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    ConvertToRGB(),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "    # ... your steps ...\n",
    "    # ... your steps ...\n",
    "    # ... your steps ...\n",
    "])\n",
    "\n",
    "print(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab54ee24-37c4-49db-a2b3-ff1d1fcafec5",
   "metadata": {},
   "source": [
    "**Task 1.5.4:**\n",
    "\n",
    "Test that the transformer pipeline is working.  Load in the specified training image and transform it.  Check that you get a 3$\\times$224$\\times$224 tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd0cf2cf-912a-4b20-8897-99398c763050",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\WANG ZHICHONG\\\\Downloads\\\\competition_VfIpjyh\\\\sea_creatures\\\\train\\\\Dolphin\\\\10004986625_0f786ab86b_b.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m sample_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msea_creatures/train/Dolphin/10004986625_0f786ab86b_b.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_file\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m# load your image\u001b[39;00m\n\u001b[0;32m      5\u001b[0m transformed_image \u001b[38;5;241m=\u001b[39m transform(image)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(transformed_image\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\PIL\\Image.py:3431\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3432\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\WANG ZHICHONG\\\\Downloads\\\\competition_VfIpjyh\\\\sea_creatures\\\\train\\\\Dolphin\\\\10004986625_0f786ab86b_b.jpg'"
     ]
    }
   ],
   "source": [
    "sample_file = \"sea_creatures/train/Dolphin/10004986625_0f786ab86b_b.jpg\"\n",
    "\n",
    "image = Image.open(sample_file)# load your image\n",
    "\n",
    "transformed_image = transform(image)\n",
    "print(transformed_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed216dfc-f976-4813-931d-4f4aff0fa229",
   "metadata": {},
   "source": [
    "**Task 1.5.5:**\n",
    "\n",
    "Create a `DataSet` for the training data (using the `ImageFolder` subclass). It should apply the transformer pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fc3f0f5-cb4e-4054-a7f3-3e4fb1be8903",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: 'sea_creatures\\\\train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage size\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\datasets\\folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    321\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    327\u001b[0m ):\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\datasets\\folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[0;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\datasets\\folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\datasets\\folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系统找不到指定的路径。: 'sea_creatures\\\\train'"
     ]
    }
   ],
   "source": [
    "dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "print(\"Image size\", dataset[0][0].shape)\n",
    "print(\"Label\", dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c23363-b7ab-4255-a651-e11e05a27dd9",
   "metadata": {},
   "source": [
    "**Task 1.5.6:**\n",
    "\n",
    "Calculate the class distribution.  Store this in the variable `class_distribution` as a dictionary.  The keys should be the class names.  The values should be the number of training samples for the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc94e1c9-1e74-4619-b240-d008bd8b4492",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# This will get us the counts, but notice that the keys are the class indices,\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# not the class names.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m counts \u001b[38;5;241m=\u001b[39m Counter(x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mdataset\u001b[49m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe counts dictionary:\u001b[39m\u001b[38;5;124m\"\u001b[39m, counts)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# This dictionary maps class names to their index.\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# This will get us the counts, but notice that the keys are the class indices,\n",
    "# not the class names.\n",
    "counts = Counter(x[1] for x in tqdm(dataset))\n",
    "print(\"The counts dictionary:\", counts)\n",
    "\n",
    "# This dictionary maps class names to their index.\n",
    "print(\"The class_to_idx dictionary:\", dataset.class_to_idx)\n",
    "\n",
    "# Use both of these to construct the desired dictionary\n",
    "idx_to_class = {v:k for k,v in dataset.class_to_idx.items()}\n",
    "\n",
    "class_distribution = {idx_to_class[idx]:count for idx, count in counts.items()}\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd36dd8-b080-40e5-a07e-247a44b452e4",
   "metadata": {},
   "source": [
    "## Transform the Data\n",
    "**Task 1.5.7:**\n",
    "\n",
    "Create a `DataLoader` that loads from this `DataSet` in batches of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "453d08c0-48f5-4e86-9884-211276557180",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m----> 2\u001b[0m dataset_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mdataset\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Get one batch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m first_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataset_loader))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "dataset_loader = DataLoader(dataset, batch_size=32,shuffle=True)\n",
    "\n",
    "# Get one batch\n",
    "first_batch = next(iter(dataset_loader))\n",
    "\n",
    "print(f\"Shape of one batch: {first_batch[0].shape}\")\n",
    "print(f\"Shape of labels: {first_batch[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda7d7d-fa68-46a1-8c7f-8909a6a66634",
   "metadata": {},
   "source": [
    "**Task 1.5.8:** Calculate the mean and standard deviation of each channel in this data set.\n",
    "\n",
    "Fill the missing lines in the `get_mean_std` function and invoke it with the right dataset.\n",
    "\n",
    "This will calculate the correct values for `mean` and `std`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f3304b7-d957-4cfb-ba1c-ce597da949c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 22\u001b[0m\n\u001b[0;32m     17\u001b[0m     std \u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39msqrt(channels_squared_sum \u001b[38;5;241m/\u001b[39m num_batches \u001b[38;5;241m-\u001b[39m mean\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mean, std\n\u001b[1;32m---> 22\u001b[0m mean, std \u001b[38;5;241m=\u001b[39m get_mean_std(\u001b[43mdataset_loader\u001b[49m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStandard deviation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_loader' is not defined"
     ]
    }
   ],
   "source": [
    "def get_mean_std(loader):\n",
    "    \"\"\"Computes the mean and standard deviation of image data.\n",
    "\n",
    "    Input: a `DataLoader` producing tensors of shape [batch_size, channels, pixels_x, pixels_y]\n",
    "    Output: the mean of each channel as a tensor, the standard deviation of each channel as a tensor\n",
    "            formatted as a tuple (means[channels], std[channels])\"\"\"\n",
    "\n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "    for data, _ in tqdm(loader):\n",
    "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "        channels_squared_sum += torch.mean(data**2, dim=[0, 2, 3])\n",
    "        num_batches += 1\n",
    "    # Compute the mean from the channels_sum and num_batches\n",
    "    mean = channels_sum / num_batches\n",
    "    # Compute the standard deviation form channels_squared_sum, num_batches,\n",
    "    # and the mean.\n",
    "    std =torch.sqrt(channels_squared_sum / num_batches - mean**2)\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "mean, std = get_mean_std(dataset_loader)\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Standard deviation: {std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33d8a69-6445-4683-97ff-37685e1facd4",
   "metadata": {},
   "source": [
    "**Task 1.5.9:**\n",
    "\n",
    "Build a new transformer pipeline that normalizes the channels according to the mean and standard deviation above. The pipeline should be assigned to the variable `transform_norm`. Afterwards, use the pipeline to create a normalized data set and store it in the variable `norm_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eff39d1f-274e-4e7f-82a2-607944e0973b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m transform_norm \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      2\u001b[0m     ConvertToRGB(),\n\u001b[0;32m      3\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m)),\n\u001b[0;32m      4\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m----> 5\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m\u001b[43mmean\u001b[49m,std\u001b[38;5;241m=\u001b[39mstd)\n\u001b[0;32m      6\u001b[0m ])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(transform_norm)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mean' is not defined"
     ]
    }
   ],
   "source": [
    "transform_norm = transforms.Compose([\n",
    "    ConvertToRGB(),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean,std=std)\n",
    "])\n",
    "print(transform_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5fc7e10-456d-460e-847b-c15867bbbe78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transform_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m norm_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(train_dir,transform\u001b[38;5;241m=\u001b[39m\u001b[43mtransform_norm\u001b[49m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage size\u001b[39m\u001b[38;5;124m\"\u001b[39m, norm_dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, norm_dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transform_norm' is not defined"
     ]
    }
   ],
   "source": [
    "norm_dataset = datasets.ImageFolder(train_dir,transform=transform_norm)\n",
    "\n",
    "print(\"Image size\", norm_dataset[0][0].shape)\n",
    "print(\"Label\", norm_dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4573dc1-52b0-488c-a151-1e9b81c6a311",
   "metadata": {},
   "source": [
    "**Task 1.5.10:**\n",
    "\n",
    "Split the normalized data set into a training set and a validation set. 80% of the data should be in the training set, and 20% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7144959b-a2c1-46cc-b6af-c6e62ca89293",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'norm_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset, val_dataset \u001b[38;5;241m=\u001b[39m random_split(\u001b[43mnorm_dataset\u001b[49m,[\u001b[38;5;241m0.8\u001b[39m,\u001b[38;5;241m0.2\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining data set size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_dataset))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation data set size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(val_dataset))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'norm_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = random_split(norm_dataset,[0.8,0.2])\n",
    "\n",
    "print(\"Training data set size:\", len(train_dataset))\n",
    "print(\"Validation data set size:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4d213e-ecf1-4f98-b7e5-e65478cf0e0a",
   "metadata": {},
   "source": [
    "**Task 1.5.11:**\n",
    "\n",
    "Set up data loaders for both the training and validation data sets.  Use the same batch size as before.  Remember to set `shuffle=True` on the training loader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "493914e4-5bd3-4385-bc9a-959b0d58d271",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mtrain_dataset\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0babc3-1b8c-4e1d-9d9e-5791f313a1a2",
   "metadata": {},
   "source": [
    "### Build the Model\n",
    "**Task 1.5.12:**\n",
    "\n",
    "Start setting up the network.  We'll begin with three layers:\n",
    "- 2D convolution with sixteen $3\\times3$ kernels\n",
    "- ReLU activation\n",
    "- Max pooling with $4\\times4$ kernels (and a stride of $4\\times4$)\n",
    "- <div class=\"alert alert-info\" role=\"alert\">\n",
    "We are asking you to assembling the model in several cells.  This helps us check that you're on the right track, and it help you see the sizes of the layers more easily.  However, if you make a mistake in a following cell that appends to the model, you need to re-run the cells from the model definition again.  If you don't, you'll be appending the working layers after the broken layers, which won't work at all!\n",
    "\n",
    "\n",
    "<br><br>\n",
    "<strong>Especially, since in the model creation you're invoking the <code>torch.manual_seed</code> methods that will ensure the same results regarding of when you're running this assignment.</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "842106a1-b3d1-4c09-988f-902767f60a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [32, 16, 56, 56]          --\n",
       "├─Conv2d: 1-1                            [32, 16, 224, 224]        448\n",
       "├─ReLU: 1-2                              [32, 16, 224, 224]        --\n",
       "├─MaxPool2d: 1-3                         [32, 16, 56, 56]          --\n",
       "==========================================================================================\n",
       "Total params: 448\n",
       "Trainable params: 448\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 719.32\n",
       "==========================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 205.52\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 224.79\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3,out_channels=16,kernel_size=3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=4, stride =4)\n",
    ")\n",
    "# ... your layers here ...\n",
    "summary(model, input_size=(batch_size, 3, height, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d8f1e2-fa5d-4d3a-80fa-7c79267e0f37",
   "metadata": {},
   "source": [
    "**Task 1.5.13:** Add three more layers to the network.\n",
    "\n",
    "- 2D convolution with thirty-two $3\\times3$ kernels\n",
    "- ReLU activation\n",
    "- Max pooling with $4\\times4$ kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b528a1e-9684-4718-8507-64893db480e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [32, 32, 14, 14]          --\n",
       "├─Conv2d: 1-1                            [32, 16, 224, 224]        448\n",
       "├─ReLU: 1-2                              [32, 16, 224, 224]        --\n",
       "├─MaxPool2d: 1-3                         [32, 16, 56, 56]          --\n",
       "├─Conv2d: 1-4                            [32, 32, 56, 56]          4,640\n",
       "├─ReLU: 1-5                              [32, 32, 56, 56]          --\n",
       "├─MaxPool2d: 1-6                         [32, 32, 14, 14]          --\n",
       "==========================================================================================\n",
       "Total params: 5,088\n",
       "Trainable params: 5,088\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.18\n",
       "==========================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 231.21\n",
       "Params size (MB): 0.02\n",
       "Estimated Total Size (MB): 250.50\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add these layers to the model\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3,out_channels=16,kernel_size=3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=4, stride =4),\n",
    "    nn.Conv2d(in_channels=16, out_channels=32,kernel_size=3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=4)\n",
    ")\n",
    "summary(model, input_size=(batch_size, 3, height, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3bba7e-a7ee-477a-8b26-d9fd776bac4d",
   "metadata": {},
   "source": [
    "**Task 1.5.14:** Add four more layers to the network.\n",
    "- 2D convolution with sixty-four $3\\times3$ kernels\n",
    "- ReLU activation\n",
    "- Max pooling with $4\\times4$ kernels\n",
    "- Flattening\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d847158-4124-4438-81c3-d258ee76a352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [32, 576]                 --\n",
       "├─Conv2d: 1-1                            [32, 16, 224, 224]        448\n",
       "├─ReLU: 1-2                              [32, 16, 224, 224]        --\n",
       "├─MaxPool2d: 1-3                         [32, 16, 56, 56]          --\n",
       "├─Conv2d: 1-4                            [32, 32, 56, 56]          4,640\n",
       "├─ReLU: 1-5                              [32, 32, 56, 56]          --\n",
       "├─MaxPool2d: 1-6                         [32, 32, 14, 14]          --\n",
       "├─Conv2d: 1-7                            [32, 64, 14, 14]          18,496\n",
       "├─ReLU: 1-8                              [32, 64, 14, 14]          --\n",
       "├─MaxPool2d: 1-9                         [32, 64, 3, 3]            --\n",
       "├─Flatten: 1-10                          [32, 576]                 --\n",
       "==========================================================================================\n",
       "Total params: 23,584\n",
       "Trainable params: 23,584\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.30\n",
       "==========================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 234.42\n",
       "Params size (MB): 0.09\n",
       "Estimated Total Size (MB): 253.78\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the new layers\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3,out_channels=16,kernel_size=3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=4, stride =4),\n",
    "    nn.Conv2d(in_channels=16, out_channels=32,kernel_size=3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=4),\n",
    "    nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=4),\n",
    "    nn.Flatten()\n",
    ")\n",
    "summary(model, input_size=(batch_size, 3, height, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a91936b-dbf6-40b0-b725-d7749c1a7597",
   "metadata": {},
   "source": [
    "**Task 1.5.15:** Add the final layers to the model\n",
    "- Drop-out (with $p=0.5$)\n",
    "- Linear layer with $500$ outputs (check the summary above to get the correct number of inputs)\n",
    "- ReLU activation\n",
    "- Drop-out\n",
    "- Linear output layer with the appropriate number of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "965fad05-6b29-4be1-918b-0bffad5fca2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [32, 9]                   --\n",
       "├─Conv2d: 1-1                            [32, 16, 224, 224]        448\n",
       "├─ReLU: 1-2                              [32, 16, 224, 224]        --\n",
       "├─MaxPool2d: 1-3                         [32, 16, 56, 56]          --\n",
       "├─Conv2d: 1-4                            [32, 32, 56, 56]          4,640\n",
       "├─ReLU: 1-5                              [32, 32, 56, 56]          --\n",
       "├─MaxPool2d: 1-6                         [32, 32, 14, 14]          --\n",
       "├─Conv2d: 1-7                            [32, 64, 14, 14]          18,496\n",
       "├─ReLU: 1-8                              [32, 64, 14, 14]          --\n",
       "├─MaxPool2d: 1-9                         [32, 64, 3, 3]            --\n",
       "├─Flatten: 1-10                          [32, 576]                 --\n",
       "├─Dropout: 1-11                          [32, 576]                 --\n",
       "├─Linear: 1-12                           [32, 500]                 288,500\n",
       "├─ReLU: 1-13                             [32, 500]                 --\n",
       "├─Dropout: 1-14                          [32, 500]                 --\n",
       "├─Linear: 1-15                           [32, 9]                   4,509\n",
       "==========================================================================================\n",
       "Total params: 316,593\n",
       "Trainable params: 316,593\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.31\n",
       "==========================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 234.55\n",
       "Params size (MB): 1.27\n",
       "Estimated Total Size (MB): 255.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the final layers\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3,out_channels=16,kernel_size=3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=4, stride =4),\n",
    "    nn.Conv2d(in_channels=16, out_channels=32,kernel_size=3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=4),\n",
    "    nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=4),\n",
    "    nn.Flatten(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=576,out_features=500),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=500,out_features=9)\n",
    ")\n",
    "summary(model, input_size=(batch_size, 3, height, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1890ec0-e490-4cc3-8a91-31697e0a2b4a",
   "metadata": {},
   "source": [
    "**Task 1.5.16:**\n",
    "\n",
    "Prepare for training. Define the loss function, create an optimizers, and send the model to the GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3ed1bd8-ba03-486d-81a8-f7629b8e39f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, maia, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: gpu",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(params\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(device)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, maia, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: gpu"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"gpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(device)\n",
    "# Send the model to the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ebf1c5-cdbf-4703-b8b1-7ca022c30d15",
   "metadata": {},
   "source": [
    "**Task 1.5.17:**\n",
    "\n",
    "Train model for $10$ epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7afc1674-273f-440d-b199-41001bc24934",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'training'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import the train and predict functions from `training.py`, instead of typing them out!\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train, predict\n\u001b[0;32m      3\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model for 10 epochs\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'training'"
     ]
    }
   ],
   "source": [
    "# Import the train and predict functions from `training.py`, instead of typing them out!\n",
    "from training import train, predict\n",
    "epochs = 10\n",
    "# Train the model for 10 epochs\n",
    "train(model, optimizer, loss_fn, train_loader, val_loader, epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b62a38-23bd-4859-90df-32f64605ede5",
   "metadata": {},
   "source": [
    "### Evaluate the Model Performance\n",
    "**Task 1.5.18:**\n",
    "\n",
    "Make predictions for all of the images in the validation set. Start by calculating the probabilities using the `predict` function (from our `training.py` module). And then calculate the predicted class based in the `probabilities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9f15628-3520-4958-93f1-211f6824b081",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Compute the probabilities for each validation image\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m(model,val_loader,device)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Get the index associated with the largest probability for each\u001b[39;00m\n\u001b[0;32m      5\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(probabilities,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute the probabilities for each validation image\n",
    "probabilities = predict(model,val_loader,device)\n",
    "\n",
    "# Get the index associated with the largest probability for each\n",
    "predictions = torch.argmax(probabilities,dim=1)\n",
    "\n",
    "print(\"Number of predictions:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85917659-cd46-41f9-b824-84c6aced8522",
   "metadata": {},
   "source": [
    "**Task 1.5.19:**\n",
    "\n",
    "Create the confusion matrix for the predictions on the validation set.  We have provided the actual classes in the `targets` variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "427dd436-dc6d-427a-8528-f8e4bd2e0aa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m targets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mval_loader\u001b[49m):\n\u001b[0;32m      4\u001b[0m     targets\u001b[38;5;241m.\u001b[39mextend(labels\u001b[38;5;241m.\u001b[39mtolist())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_loader' is not defined"
     ]
    }
   ],
   "source": [
    "targets = []\n",
    "\n",
    "for _, labels in tqdm(val_loader):\n",
    "    targets.extend(labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e31e071-31d9-4aeb-9c3c-c3fca97723ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Don't change this\u001b[39;00m\n\u001b[0;32m      2\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 4\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(torch\u001b[38;5;241m.\u001b[39mtensor(targets)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mpredictions\u001b[49m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Get the class names\u001b[39;00m\n\u001b[0;32m      7\u001b[0m classes \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(train_dir)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Don't change this\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "cm = confusion_matrix(torch.tensor(targets).cpu().numpy(), torch.tensor(predictions).cpu().numpy())\n",
    "\n",
    "# Get the class names\n",
    "classes = os.listdir(train_dir)\n",
    "\n",
    "# Display the confusion matrix (don't change this)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation=\"vertical\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb223b-d9e2-46e3-88ae-ec09f93b16e7",
   "metadata": {},
   "source": [
    "**Task 1.5.20:** \n",
    "\n",
    "Create a data set for the test data.  It is located in the `sea_creatures/test` directory.  Then create a data loader from this data set.  _DO NOT_ shuffle this data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "559e9ca2-4e9d-4f11-921d-3b786127affb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Predict the probabilities for each test image\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_probabilities \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m(model, test_loader, device)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Get the index associated with the largest probability for each test image\u001b[39;00m\n\u001b[0;32m      5\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(test_probabilities,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "source": [
    "# Predict the probabilities for each test image\n",
    "test_probabilities = predict(model, test_loader, device)\n",
    "\n",
    "# Get the index associated with the largest probability for each test image\n",
    "test_predictions = torch.argmax(test_probabilities,dim=1)\n",
    "\n",
    "print(\"Number of predictions:\", test_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdacee7-af4d-4538-8d17-53e468f5bd3f",
   "metadata": {},
   "source": [
    "**Task 1.5.22:** Convert the class index to the class name for each test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a138f8f5-668c-4275-9f75-970f1b67e1bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_classes \u001b[38;5;241m=\u001b[39m [classes[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtest_predictions\u001b[49m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of class predictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(test_classes))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "test_classes = [classes[i] for i in test_predictions]\n",
    "\n",
    "print(\"Number of class predictions:\", len(test_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4a4818-09c3-4d63-ab9a-07e69210a8a8",
   "metadata": {},
   "source": [
    "### Final checks\n",
    "\n",
    "You can now check how accurate your model is by sampling some images from our `/test` directory. These images are not labled, so you'll need to check it manually.\n",
    "\n",
    "The code below randomly samples 12 images from the test directory and shows them in a grid alongside its predicted label. Run it as many times as you want to get different samples.\n",
    "\n",
    "How is it working?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4c885e6-e2e3-436c-ac11-101ddc7880cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Sample 12 random indices from the test dataset\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m sample_indices \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtest_loader\u001b[49m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39msamples)), \u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Create a grid of 4x3 subplots\u001b[39;00m\n\u001b[0;32m      8\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Sample 12 random indices from the test dataset\n",
    "sample_indices = random.sample(range(len(test_loader.dataset.samples)), 12)\n",
    "\n",
    "# Create a grid of 4x3 subplots\n",
    "fig, axes = plt.subplots(4, 3, figsize=(20, 10))\n",
    "\n",
    "# Iterate over the sampled indices and plot the corresponding images\n",
    "for ax, idx in zip(axes.flatten(), sample_indices):\n",
    "    image_path = test_loader.dataset.samples[idx][0]\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Display the image on the axis\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Get the predicted class for this image\n",
    "    predicted_class = test_classes[idx]\n",
    "\n",
    "    # Set the title of the subplot to the predicted class\n",
    "    ax.set_title(f\"Predicted: {predicted_class}\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfc490b-e2db-4c33-98ba-dabef079307f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
